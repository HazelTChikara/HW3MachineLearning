{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2930ee73",
   "metadata": {},
   "source": [
    "# Task 2: Movie Recommender System\n",
    "## Machine Learning with Matrix Data for Recommender Systems\n",
    "\n",
    "**Author:** HazelTChikara  \n",
    "**Date:** November 23, 2025\n",
    "\n",
    "This notebook implements and evaluates three recommender system algorithms:\n",
    "1. Probabilistic Matrix Factorization (PMF)\n",
    "2. User-based Collaborative Filtering\n",
    "3. Item-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d17c45",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570de904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "# !pip install pandas numpy matplotlib seaborn scikit-surprise scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36182b6f",
   "metadata": {},
   "source": [
    "### Important: Installing scikit-surprise\n",
    "\n",
    "**Python 3.13 Compatibility Issue:**\n",
    "The `scikit-surprise` library currently has compatibility issues with Python 3.13.\n",
    "\n",
    "**Recommended Solutions:**\n",
    "\n",
    "1. **Use Conda (Recommended):**\n",
    "   ```bash\n",
    "   conda install -c conda-forge scikit-surprise\n",
    "   ```\n",
    "\n",
    "2. **Use Python 3.9-3.12:**\n",
    "   - Create a virtual environment with Python 3.9, 3.10, 3.11, or 3.12\n",
    "   - Then install: `pip install scikit-surprise`\n",
    "\n",
    "3. **Try terminal installation:**\n",
    "   ```bash\n",
    "   pip install scikit-surprise\n",
    "   ```\n",
    "\n",
    "4. **Alternative for Python 3.13:**\n",
    "   ```bash\n",
    "   # Install from git (development version)\n",
    "   pip install git+https://github.com/NicolasHug/Surprise.git\n",
    "   ```\n",
    "\n",
    "**Check your Python version first:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8653cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q scikit-surprise\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from surprise import Dataset, Reader, SVD, KNNBasic\n",
    "from surprise.model_selection import cross_validate\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec6847",
   "metadata": {},
   "source": [
    "## Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ratings data\n",
    "print(\"Loading data from: ratings_small.csv\")\n",
    "\n",
    "df = pd.read_csv('ratings_small.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166482b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "print(f\"Dataset Statistics:\")\n",
    "print(f\"Number of users: {df['userId'].nunique()}\")\n",
    "print(f\"Number of movies: {df['movieId'].nunique()}\")\n",
    "print(f\"Number of ratings: {len(df)}\")\n",
    "print(f\"Rating range: [{df['rating'].min()}, {df['rating'].max()}]\")\n",
    "print(f\"Average rating: {df['rating'].mean():.2f}\")\n",
    "print(f\"\\nRating distribution:\")\n",
    "df['rating'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511724f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rating distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "df['rating'].hist(bins=10, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Rating', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Movie Ratings', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20627200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Surprise dataset\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data = Dataset.load_from_df(df[['userId', 'movieId', 'rating']], reader)\n",
    "print(\"Surprise dataset created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de399fd2",
   "metadata": {},
   "source": [
    "## Task 2c: 5-Fold Cross-Validation Comparison\n",
    "\n",
    "Compute average MAE and RMSE for:\n",
    "1. Probabilistic Matrix Factorization (PMF)\n",
    "2. User-based Collaborative Filtering\n",
    "3. Item-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TASK 2C: 5-Fold Cross-Validation Comparison\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Probabilistic Matrix Factorization (PMF) - using SVD\n",
    "print(\"\\n1. Probabilistic Matrix Factorization (SVD)...\")\n",
    "pmf = SVD()\n",
    "pmf_results = cross_validate(pmf, data, measures=['MAE', 'RMSE'], cv=5, verbose=True)\n",
    "results['PMF'] = {\n",
    "    'MAE': pmf_results['test_mae'].mean(),\n",
    "    'RMSE': pmf_results['test_rmse'].mean(),\n",
    "    'MAE_std': pmf_results['test_mae'].std(),\n",
    "    'RMSE_std': pmf_results['test_rmse'].std()\n",
    "}\n",
    "print(f\"MAE: {results['PMF']['MAE']:.4f} ± {results['PMF']['MAE_std']:.4f}\")\n",
    "print(f\"RMSE: {results['PMF']['RMSE']:.4f} ± {results['PMF']['RMSE_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f064fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. User-based Collaborative Filtering\n",
    "print(\"\\n2. User-based Collaborative Filtering...\")\n",
    "user_cf = KNNBasic(sim_options={'name': 'cosine', 'user_based': True})\n",
    "user_results = cross_validate(user_cf, data, measures=['MAE', 'RMSE'], cv=5, verbose=True)\n",
    "results['User-based CF'] = {\n",
    "    'MAE': user_results['test_mae'].mean(),\n",
    "    'RMSE': user_results['test_rmse'].mean(),\n",
    "    'MAE_std': user_results['test_mae'].std(),\n",
    "    'RMSE_std': user_results['test_rmse'].std()\n",
    "}\n",
    "print(f\"MAE: {results['User-based CF']['MAE']:.4f} ± {results['User-based CF']['MAE_std']:.4f}\")\n",
    "print(f\"RMSE: {results['User-based CF']['RMSE']:.4f} ± {results['User-based CF']['RMSE_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253df2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Item-based Collaborative Filtering\n",
    "print(\"\\n3. Item-based Collaborative Filtering...\")\n",
    "item_cf = KNNBasic(sim_options={'name': 'cosine', 'user_based': False})\n",
    "item_results = cross_validate(item_cf, data, measures=['MAE', 'RMSE'], cv=5, verbose=True)\n",
    "results['Item-based CF'] = {\n",
    "    'MAE': item_results['test_mae'].mean(),\n",
    "    'RMSE': item_results['test_rmse'].mean(),\n",
    "    'MAE_std': item_results['test_mae'].std(),\n",
    "    'RMSE_std': item_results['test_rmse'].std()\n",
    "}\n",
    "print(f\"MAE: {results['Item-based CF']['MAE']:.4f} ± {results['Item-based CF']['MAE_std']:.4f}\")\n",
    "print(f\"RMSE: {results['Item-based CF']['RMSE']:.4f} ± {results['Item-based CF']['RMSE_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf52e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SUMMARY OF RESULTS:\")\n",
    "print(\"-\"*80)\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df[['MAE', 'RMSE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34695d9f",
   "metadata": {},
   "source": [
    "## Task 2d: Model Comparison and Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebdc1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model\n",
    "comparison_df = pd.DataFrame(results).T\n",
    "best_mae_model = comparison_df['MAE'].idxmin()\n",
    "best_rmse_model = comparison_df['RMSE'].idxmin()\n",
    "\n",
    "print(f\"Best Model by MAE: {best_mae_model} (MAE = {comparison_df.loc[best_mae_model, 'MAE']:.4f})\")\n",
    "print(f\"Best Model by RMSE: {best_rmse_model} (RMSE = {comparison_df.loc[best_rmse_model, 'RMSE']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0085aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "algorithms = list(results.keys())\n",
    "mae_values = [results[algo]['MAE'] for algo in algorithms]\n",
    "mae_std = [results[algo]['MAE_std'] for algo in algorithms]\n",
    "rmse_values = [results[algo]['RMSE'] for algo in algorithms]\n",
    "rmse_std = [results[algo]['RMSE_std'] for algo in algorithms]\n",
    "\n",
    "# MAE comparison\n",
    "axes[0].bar(algorithms, mae_values, yerr=mae_std, capsize=5, alpha=0.7, \n",
    "            color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[0].set_ylabel('MAE', fontsize=12)\n",
    "axes[0].set_title('Mean Absolute Error (MAE) Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim(bottom=0)\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "for i, (v, s) in enumerate(zip(mae_values, mae_std)):\n",
    "    axes[0].text(i, v + s + 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[1].bar(algorithms, rmse_values, yerr=rmse_std, capsize=5, alpha=0.7, \n",
    "            color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[1].set_ylabel('RMSE', fontsize=12)\n",
    "axes[1].set_title('Root Mean Squared Error (RMSE) Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim(bottom=0)\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "for i, (v, s) in enumerate(zip(rmse_values, rmse_std)):\n",
    "    axes[1].text(i, v + s + 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('task_2d_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Plot saved as 'task_2d_model_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a74a2",
   "metadata": {},
   "source": [
    "## Task 2e: Impact of Similarity Metrics\n",
    "\n",
    "Examine how cosine, MSD, and Pearson similarities impact CF performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a2c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = ['cosine', 'msd', 'pearson']\n",
    "user_sim_results = defaultdict(dict)\n",
    "item_sim_results = defaultdict(dict)\n",
    "\n",
    "for sim in similarities:\n",
    "    print(f\"\\n--- Testing similarity: {sim.upper()} ---\")\n",
    "    \n",
    "    # User-based CF\n",
    "    print(f\"User-based CF with {sim}...\")\n",
    "    user_cf = KNNBasic(sim_options={'name': sim, 'user_based': True})\n",
    "    user_cv = cross_validate(user_cf, data, measures=['MAE', 'RMSE'], cv=5, verbose=False)\n",
    "    user_sim_results[sim]['MAE'] = user_cv['test_mae'].mean()\n",
    "    user_sim_results[sim]['RMSE'] = user_cv['test_rmse'].mean()\n",
    "    user_sim_results[sim]['MAE_std'] = user_cv['test_mae'].std()\n",
    "    user_sim_results[sim]['RMSE_std'] = user_cv['test_rmse'].std()\n",
    "    print(f\"  MAE: {user_sim_results[sim]['MAE']:.4f}, RMSE: {user_sim_results[sim]['RMSE']:.4f}\")\n",
    "    \n",
    "    # Item-based CF\n",
    "    print(f\"Item-based CF with {sim}...\")\n",
    "    item_cf = KNNBasic(sim_options={'name': sim, 'user_based': False})\n",
    "    item_cv = cross_validate(item_cf, data, measures=['MAE', 'RMSE'], cv=5, verbose=False)\n",
    "    item_sim_results[sim]['MAE'] = item_cv['test_mae'].mean()\n",
    "    item_sim_results[sim]['RMSE'] = item_cv['test_rmse'].mean()\n",
    "    item_sim_results[sim]['MAE_std'] = item_cv['test_mae'].std()\n",
    "    item_sim_results[sim]['RMSE_std'] = item_cv['test_rmse'].std()\n",
    "    print(f\"  MAE: {item_sim_results[sim]['MAE']:.4f}, RMSE: {item_sim_results[sim]['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary tables\n",
    "print(\"\\nUser-based CF:\")\n",
    "user_sim_df = pd.DataFrame(user_sim_results).T\n",
    "display(user_sim_df[['MAE', 'RMSE']])\n",
    "\n",
    "print(\"\\nItem-based CF:\")\n",
    "item_sim_df = pd.DataFrame(item_sim_results).T\n",
    "display(item_sim_df[['MAE', 'RMSE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "x_pos = np.arange(len(similarities))\n",
    "width = 0.35\n",
    "\n",
    "user_mae = [user_sim_results[sim]['MAE'] for sim in similarities]\n",
    "item_mae = [item_sim_results[sim]['MAE'] for sim in similarities]\n",
    "user_mae_std = [user_sim_results[sim]['MAE_std'] for sim in similarities]\n",
    "item_mae_std = [item_sim_results[sim]['MAE_std'] for sim in similarities]\n",
    "user_rmse = [user_sim_results[sim]['RMSE'] for sim in similarities]\n",
    "item_rmse = [item_sim_results[sim]['RMSE'] for sim in similarities]\n",
    "user_rmse_std = [user_sim_results[sim]['RMSE_std'] for sim in similarities]\n",
    "item_rmse_std = [item_sim_results[sim]['RMSE_std'] for sim in similarities]\n",
    "\n",
    "# MAE comparison\n",
    "axes[0, 0].bar(x_pos - width/2, user_mae, width, yerr=user_mae_std, label='User-based CF', \n",
    "               capsize=5, alpha=0.8, color='#1f77b4')\n",
    "axes[0, 0].bar(x_pos + width/2, item_mae, width, yerr=item_mae_std, label='Item-based CF', \n",
    "               capsize=5, alpha=0.8, color='#ff7f0e')\n",
    "axes[0, 0].set_ylabel('MAE', fontsize=12)\n",
    "axes[0, 0].set_title('MAE: User-based vs Item-based CF', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels([s.upper() for s in similarities])\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0, 1].bar(x_pos - width/2, user_rmse, width, yerr=user_rmse_std, label='User-based CF', \n",
    "               capsize=5, alpha=0.8, color='#1f77b4')\n",
    "axes[0, 1].bar(x_pos + width/2, item_rmse, width, yerr=item_rmse_std, label='Item-based CF', \n",
    "               capsize=5, alpha=0.8, color='#ff7f0e')\n",
    "axes[0, 1].set_ylabel('RMSE', fontsize=12)\n",
    "axes[0, 1].set_title('RMSE: User-based vs Item-based CF', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xticks(x_pos)\n",
    "axes[0, 1].set_xticklabels([s.upper() for s in similarities])\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# User-based CF trends\n",
    "axes[1, 0].plot(similarities, user_mae, 'o-', label='MAE', linewidth=2, markersize=8, color='#2ca02c')\n",
    "axes[1, 0].plot(similarities, user_rmse, 's-', label='RMSE', linewidth=2, markersize=8, color='#d62728')\n",
    "axes[1, 0].set_xlabel('Similarity Metric', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Error', fontsize=12)\n",
    "axes[1, 0].set_title('User-based CF: Similarity Impact', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Item-based CF trends\n",
    "axes[1, 1].plot(similarities, item_mae, 'o-', label='MAE', linewidth=2, markersize=8, color='#2ca02c')\n",
    "axes[1, 1].plot(similarities, item_rmse, 's-', label='RMSE', linewidth=2, markersize=8, color='#d62728')\n",
    "axes[1, 1].set_xlabel('Similarity Metric', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Error', fontsize=12)\n",
    "axes[1, 1].set_title('Item-based CF: Similarity Impact', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('task_2e_similarity_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Plot saved as 'task_2e_similarity_metrics.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2cacbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "user_best_mae = min(similarities, key=lambda s: user_sim_results[s]['MAE'])\n",
    "user_best_rmse = min(similarities, key=lambda s: user_sim_results[s]['RMSE'])\n",
    "item_best_mae = min(similarities, key=lambda s: item_sim_results[s]['MAE'])\n",
    "item_best_rmse = min(similarities, key=lambda s: item_sim_results[s]['RMSE'])\n",
    "\n",
    "print(f\"Best similarity for User-based CF: {user_best_mae.upper()} (MAE), {user_best_rmse.upper()} (RMSE)\")\n",
    "print(f\"Best similarity for Item-based CF: {item_best_mae.upper()} (MAE), {item_best_rmse.upper()} (RMSE)\")\n",
    "\n",
    "if user_best_mae == item_best_mae and user_best_rmse == item_best_rmse:\n",
    "    print(f\"\\nThe impact of similarity metrics is CONSISTENT between User-based and Item-based CF.\")\n",
    "else:\n",
    "    print(f\"\\nThe impact of similarity metrics is NOT CONSISTENT between User-based and Item-based CF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79ef420",
   "metadata": {},
   "source": [
    "## Task 2f: Impact of Number of Neighbors\n",
    "\n",
    "Examine how different k values affect CF performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62620c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [5, 10, 20, 30, 40, 50, 60, 70, 80]\n",
    "user_neighbor_results = defaultdict(dict)\n",
    "item_neighbor_results = defaultdict(dict)\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\n--- Testing k = {k} ---\")\n",
    "    \n",
    "    # User-based CF\n",
    "    print(f\"User-based CF with k={k}...\")\n",
    "    user_cf = KNNBasic(k=k, sim_options={'name': 'cosine', 'user_based': True})\n",
    "    user_cv = cross_validate(user_cf, data, measures=['MAE', 'RMSE'], cv=5, verbose=False)\n",
    "    user_neighbor_results[k]['MAE'] = user_cv['test_mae'].mean()\n",
    "    user_neighbor_results[k]['RMSE'] = user_cv['test_rmse'].mean()\n",
    "    user_neighbor_results[k]['MAE_std'] = user_cv['test_mae'].std()\n",
    "    user_neighbor_results[k]['RMSE_std'] = user_cv['test_rmse'].std()\n",
    "    print(f\"  MAE: {user_neighbor_results[k]['MAE']:.4f}, RMSE: {user_neighbor_results[k]['RMSE']:.4f}\")\n",
    "    \n",
    "    # Item-based CF\n",
    "    print(f\"Item-based CF with k={k}...\")\n",
    "    item_cf = KNNBasic(k=k, sim_options={'name': 'cosine', 'user_based': False})\n",
    "    item_cv = cross_validate(item_cf, data, measures=['MAE', 'RMSE'], cv=5, verbose=False)\n",
    "    item_neighbor_results[k]['MAE'] = item_cv['test_mae'].mean()\n",
    "    item_neighbor_results[k]['RMSE'] = item_cv['test_rmse'].mean()\n",
    "    item_neighbor_results[k]['MAE_std'] = item_cv['test_mae'].std()\n",
    "    item_neighbor_results[k]['RMSE_std'] = item_cv['test_rmse'].std()\n",
    "    print(f\"  MAE: {item_neighbor_results[k]['MAE']:.4f}, RMSE: {item_neighbor_results[k]['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary tables\n",
    "print(\"\\nUser-based CF:\")\n",
    "user_neighbor_df = pd.DataFrame(user_neighbor_results).T\n",
    "display(user_neighbor_df[['MAE', 'RMSE']])\n",
    "\n",
    "print(\"\\nItem-based CF:\")\n",
    "item_neighbor_df = pd.DataFrame(item_neighbor_results).T\n",
    "display(item_neighbor_df[['MAE', 'RMSE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc29b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "user_mae = [user_neighbor_results[k]['MAE'] for k in k_values]\n",
    "user_rmse = [user_neighbor_results[k]['RMSE'] for k in k_values]\n",
    "item_mae = [item_neighbor_results[k]['MAE'] for k in k_values]\n",
    "item_rmse = [item_neighbor_results[k]['RMSE'] for k in k_values]\n",
    "\n",
    "# User-based CF\n",
    "axes[0].plot(k_values, user_mae, 'o-', label='MAE', linewidth=2, markersize=8, color='#2ca02c')\n",
    "axes[0].plot(k_values, user_rmse, 's-', label='RMSE', linewidth=2, markersize=8, color='#d62728')\n",
    "axes[0].set_xlabel('Number of Neighbors (k)', fontsize=12)\n",
    "axes[0].set_ylabel('Error', fontsize=12)\n",
    "axes[0].set_title('User-based CF: Impact of k', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_xticks(k_values)\n",
    "\n",
    "# Item-based CF\n",
    "axes[1].plot(k_values, item_mae, 'o-', label='MAE', linewidth=2, markersize=8, color='#2ca02c')\n",
    "axes[1].plot(k_values, item_rmse, 's-', label='RMSE', linewidth=2, markersize=8, color='#d62728')\n",
    "axes[1].set_xlabel('Number of Neighbors (k)', fontsize=12)\n",
    "axes[1].set_ylabel('Error', fontsize=12)\n",
    "axes[1].set_title('Item-based CF: Impact of k', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_xticks(k_values)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('task_2f_neighbor_impact.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Plot saved as 'task_2f_neighbor_impact.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80521dd",
   "metadata": {},
   "source": [
    "## Task 2g: Identifying Best K Value\n",
    "\n",
    "Find the optimal number of neighbors for both CF methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a8806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best k for each method based on RMSE\n",
    "user_best_k = min(k_values, key=lambda k: user_neighbor_results[k]['RMSE'])\n",
    "item_best_k = min(k_values, key=lambda k: item_neighbor_results[k]['RMSE'])\n",
    "\n",
    "print(\"Best K based on RMSE:\")\n",
    "print(f\"User-based CF: k = {user_best_k} (RMSE = {user_neighbor_results[user_best_k]['RMSE']:.4f})\")\n",
    "print(f\"Item-based CF: k = {item_best_k} (RMSE = {item_neighbor_results[item_best_k]['RMSE']:.4f})\")\n",
    "\n",
    "# Also show best k based on MAE\n",
    "user_best_k_mae = min(k_values, key=lambda k: user_neighbor_results[k]['MAE'])\n",
    "item_best_k_mae = min(k_values, key=lambda k: item_neighbor_results[k]['MAE'])\n",
    "\n",
    "print(\"\\nBest K based on MAE:\")\n",
    "print(f\"User-based CF: k = {user_best_k_mae} (MAE = {user_neighbor_results[user_best_k_mae]['MAE']:.4f})\")\n",
    "print(f\"Item-based CF: k = {item_best_k_mae} (MAE = {item_neighbor_results[item_best_k_mae]['MAE']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf0a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization with best k highlighted\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# RMSE plots\n",
    "axes[0, 0].plot(k_values, user_rmse, 'o-', linewidth=2, markersize=8, color='#1f77b4', label='User-based CF')\n",
    "axes[0, 0].plot([user_best_k], [user_neighbor_results[user_best_k]['RMSE']], 'r*', \n",
    "                 markersize=20, label=f'Best k={user_best_k}')\n",
    "axes[0, 0].set_xlabel('Number of Neighbors (k)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('RMSE', fontsize=12)\n",
    "axes[0, 0].set_title('User-based CF: RMSE vs k', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "axes[0, 0].set_xticks(k_values)\n",
    "\n",
    "axes[0, 1].plot(k_values, item_rmse, 'o-', linewidth=2, markersize=8, color='#ff7f0e', label='Item-based CF')\n",
    "axes[0, 1].plot([item_best_k], [item_neighbor_results[item_best_k]['RMSE']], 'r*', \n",
    "                 markersize=20, label=f'Best k={item_best_k}')\n",
    "axes[0, 1].set_xlabel('Number of Neighbors (k)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('RMSE', fontsize=12)\n",
    "axes[0, 1].set_title('Item-based CF: RMSE vs k', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].set_xticks(k_values)\n",
    "\n",
    "# MAE plots\n",
    "axes[1, 0].plot(k_values, user_mae, 'o-', linewidth=2, markersize=8, color='#1f77b4', label='User-based CF')\n",
    "axes[1, 0].plot([user_best_k_mae], [user_neighbor_results[user_best_k_mae]['MAE']], 'r*', \n",
    "                 markersize=20, label=f'Best k={user_best_k_mae}')\n",
    "axes[1, 0].set_xlabel('Number of Neighbors (k)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('MAE', fontsize=12)\n",
    "axes[1, 0].set_title('User-based CF: MAE vs k', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "axes[1, 0].set_xticks(k_values)\n",
    "\n",
    "axes[1, 1].plot(k_values, item_mae, 'o-', linewidth=2, markersize=8, color='#ff7f0e', label='Item-based CF')\n",
    "axes[1, 1].plot([item_best_k_mae], [item_neighbor_results[item_best_k_mae]['MAE']], 'r*', \n",
    "                 markersize=20, label=f'Best k={item_best_k_mae}')\n",
    "axes[1, 1].set_xlabel('Number of Neighbors (k)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('MAE', fontsize=12)\n",
    "axes[1, 1].set_title('Item-based CF: MAE vs k', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "axes[1, 1].set_xticks(k_values)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('task_2g_best_k.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Plot saved as 'task_2g_best_k.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91872c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION:\")\n",
    "print(\"=\"*80)\n",
    "if user_best_k == item_best_k:\n",
    "    print(f\"The best k is THE SAME for both methods: k = {user_best_k}\")\n",
    "    print(\"This suggests that both user-based and item-based CF benefit from\")\n",
    "    print(\"the same neighborhood size in this dataset.\")\n",
    "else:\n",
    "    print(\"The best k is DIFFERENT for each method:\")\n",
    "    print(f\"  - User-based CF: k = {user_best_k}\")\n",
    "    print(f\"  - Item-based CF: k = {item_best_k}\")\n",
    "    print(\"This suggests that user-based and item-based CF have different\")\n",
    "    print(\"optimal neighborhood sizes for this dataset.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e752d28a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has completed all tasks:\n",
    "- **Task 2c**: Evaluated PMF, User-based CF, and Item-based CF with 5-fold CV\n",
    "- **Task 2d**: Compared models and identified the best performer\n",
    "- **Task 2e**: Analyzed impact of similarity metrics (cosine, MSD, Pearson)\n",
    "- **Task 2f**: Examined how k affects performance\n",
    "- **Task 2g**: Identified optimal k values for both CF methods\n",
    "\n",
    "All results have been saved as high-resolution PNG files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
